{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes à idées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "- notebook suivant (copié) le rapport de stage en appelant explicitement le code pour reconstruire les résultats des expéreinces et illustrer l'utilisation de la librairie.\n",
    "- plan methode:\\\n",
    "    data info et objectif/possibles modèles\n",
    "    - baseline mathématique \n",
    "    - GNN + RNN\n",
    "        - construction du graph AMP (Umap des predicteurs)\n",
    "        - modèle (maths et data process/intuition et vraie idée de cette architecture -> dessin ?)\n",
    "        - métriques de réanalyse\n",
    "    - Autres modèles (CNN, RNN + GAN) \\\n",
    "    récapitulatif de métriques basiques pour arbitrage\n",
    "- résultats: après reconstruction du dataset, métriques physiques pour la réanalyse \n",
    "\n",
    "- les __init__ peuvent contenir les import pour cohérence plus claire entre dossiers \n",
    "\n",
    "- video de la représentation latente pendant l'entrainement (grin doit approcher lentement l'optimum pour prendre avantage des correlations temporelles sans être attracté par les corrélations spatiales)\n",
    "\n",
    "- dossier de guides: training GRIN/baseline/neural net, import data, impute full dataset from trained..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning:\n",
    "\n",
    "- coder la regression linéaire en neural net pytorch pour meilleure comparaison mais aussi avoir la main sur le training (entrainer à la reconstruction des trous et pas au fit de tout le model --> loss backward après l'utilisation du mask)\n",
    "\n",
    "- training en pass sur séries complétées step après step, ou training random par complétion de step ? \n",
    "\n",
    "- différence de régime passé lointain: forecast/imputation ou out-of-sample/in-sample imputation ? \n",
    "\n",
    "- MLP entre foreward et backward pass pourrait être entrainé avec différentes distributions de masking inhomogènes (avec la distribution en predicteur?) pour ensuite le passer sur l'inhomogeneité connue du dataset \n",
    "\n",
    "- SVD imputation en autoencodeur, pour la comparaison des modèles dans le cadre machine learning\n",
    "\n",
    "- neural net spatial, LSTM temporel, tradeoff d'erreur en fonction de la quantité de data out of sample --> est ce qu'un graph neural net prend le meilleur des deux \n",
    "\n",
    "- diffusion de l'erreur en forecast pur (ou en imputation faible à différent niveaux), donc encore out of sample \n",
    "\n",
    "- cohérence physique de la reconstrution avec double corrélations: overlap avec la matrice de corrélations de la vraie donnée et donnée générée en forecast pur (ou en imputation faible à différent niveaux)\n",
    "\n",
    "- carte d'erreurs spatiales aggréhgées (comment aggréger à différentes échelles -> potentiellement pas d'échelle de corrélation dans la vraie data non plus: à checker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Théorie/météo:\n",
    "\n",
    "- Est ce que la physique du climat à une symmétrie par inversion du temps ? L'instabilité des modèles non linéaires par inversion du temps ne vient que de la sensibilité aux conditions initiales ou il y à autre chose ? (https://www.reddit.com/r/AskPhysics/comments/ot65pr/are_navier_stokes_equations_time_symmetric/?rdt=51144)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
