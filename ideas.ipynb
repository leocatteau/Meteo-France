{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes à idées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "- notebook suivant (copié) le rapport de stage en appelant explicitement le code pour reconstruire les résultats des expéreinces et illustrer l'utilisation de la librairie.\n",
    "- plan methode:\\\n",
    "    data info et objectif/possibles modèles\n",
    "    - baseline mathématique \n",
    "    - GNN + RNN\n",
    "        - construction du graph AMP (Umap des predicteurs)\n",
    "        - modèle (maths et data process/intuition et vraie idée de cette architecture -> dessin ?)\n",
    "        - métriques de réanalyse\n",
    "    - Autres modèles (CNN, RNN + GAN) \\\n",
    "    récapitulatif de métriques basiques pour arbitrage\n",
    "- résultats: après reconstruction du dataset, métriques physiques pour la réanalyse \n",
    "\n",
    "- les __init__ peuvent contenir les import pour cohérence plus claire entre dossiers \n",
    "\n",
    "- video de la représentation latente pendant l'entrainement (grin doit approcher lentement l'optimum pour prendre avantage des correlations temporelles sans être attracté par les corrélations spatiales)\n",
    "\n",
    "- dossier de guides: training GRIN/baseline/neural net, import data, impute full dataset from trained..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning:\n",
    "\n",
    "- coder la regression linéaire en neural net pytorch pour meilleure comparaison mais aussi avoir la main sur le training (entrainer à la reconstruction des trous et pas au fit de tout le model --> loss backward après l'utilisation du mask)\n",
    "\n",
    "- training en pass sur séries complétées step après step, ou training random par complétion de step ? \n",
    "\n",
    "- différence de régime passé lointain: forecast/imputation ou out-of-sample/in-sample imputation ? \n",
    "\n",
    "- MLP entre foreward et backward pass pourrait être entrainé avec différentes distributions de masking inhomogènes (avec la distribution en predicteur?) pour ensuite le passer sur l'inhomogeneité connue du dataset \n",
    "\n",
    "- SVD imputation en autoencodeur, pour la comparaison des modèles dans le cadre machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Théorie/météo:\n",
    "\n",
    "- Est ce que la physique du climat à une symmétrie par inversion du temps ? L'instabilité des modèles non linéaires par inversion du temps ne vient que de la sensibilité aux conditions initiales ou il y à autre chose ? (https://www.reddit.com/r/AskPhysics/comments/ot65pr/are_navier_stokes_equations_time_symmetric/?rdt=51144)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
